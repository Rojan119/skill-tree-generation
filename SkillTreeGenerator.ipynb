{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0e4c46",
   "metadata": {},
   "source": [
    "<h1>PDF LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea0386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_pdf_paths(root_dir: str) -> List[str]:\n",
    "    \"\"\"Get all PDF paths recursively from a directory\"\"\"\n",
    "    pdf_paths = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith(\".pdf\"):\n",
    "                full_path = os.path.join(dirpath, filename)\n",
    "                pdf_paths.append(full_path)\n",
    "    return pdf_paths\n",
    "\n",
    "# Define your directories (replace with actual paths)\n",
    "dir_a = \"/home/rojan/Music/SS/A\"\n",
    "dir_b_best = \"/home/rojan/Music/SS/B/Best\"\n",
    "dir_b_road = \"/home/rojan/Music/SS/B/Road\"\n",
    "\n",
    "# Collect all PDF paths from all directories\n",
    "all_pdf_paths = []\n",
    "all_pdf_paths += get_pdf_paths(dir_a)\n",
    "all_pdf_paths += get_pdf_paths(dir_b_best)\n",
    "all_pdf_paths += get_pdf_paths(dir_b_road)\n",
    "\n",
    "# Load all PDF documents with enhanced processing\n",
    "documents: List[Document] = []\n",
    "\n",
    "for pdf_path in all_pdf_paths:\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Initialize section tracking for this document\n",
    "        current_section = \"Introduction\"\n",
    "        source_file = os.path.basename(pdf_path)\n",
    "        \n",
    "        for doc in docs:\n",
    "            # Clean PDF artifacts and normalize checkmarks\n",
    "            cleaned_content = re.sub(r'|', '[CHECK]', doc.page_content)\n",
    "            cleaned_content = re.sub(r'\\x0c', '', cleaned_content)  # Remove form feeds\n",
    "            \n",
    "            # Detect section headers (modify pattern as needed)\n",
    "            section_match = re.search(r'^\\n([A-Z][A-Za-z ]+)\\n', cleaned_content)\n",
    "            if section_match:\n",
    "                current_section = section_match.group(1).strip()\n",
    "            \n",
    "            # Enhanced metadata\n",
    "            doc.metadata.update({\n",
    "                \"source_folder\": os.path.dirname(pdf_path),\n",
    "                \"source_file\": source_file,\n",
    "                \"page_number\": doc.metadata[\"page\"] + 1,  # Convert to 1-based numbering\n",
    "                \"section\": current_section,\n",
    "                \"content_length\": len(cleaned_content),\n",
    "                \"content_type\": \"checklist\" if \"[CHECK]\" in cleaned_content else \"explanatory\",\n",
    "                \"document_type\": \"Best\" if \"Best\" in pdf_path else \"Road\" if \"Road\" in pdf_path else \"A\"\n",
    "            })\n",
    "            \n",
    "            # Update the cleaned content\n",
    "            doc.page_content = cleaned_content\n",
    "            \n",
    "            documents.append(doc)\n",
    "            \n",
    "       \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading : {str(e)}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317243b",
   "metadata": {},
   "source": [
    "<h1>EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69c65ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 199\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Split documents while preserving metadata and context\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\n",
    "            \"\\n\\n• \",  # Checklist items\n",
    "            \"\\n\\n\",    # Major sections\n",
    "            \"\\n• \",     # Sub-items\n",
    "            \"\\n- \",     # Alternative list format\n",
    "            \"\\n\",       # New lines\n",
    "            \" \",         # Words\n",
    "            \"\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for chunk in chunks:\n",
    "            # Create new document with inherited metadata\n",
    "            metadata = doc.metadata.copy()\n",
    "            metadata.update({\n",
    "                \"chunk_id\": f\"{metadata['source_file']}-p{metadata['page_number']}-{len(split_docs)}\",\n",
    "                \"content_length\": len(chunk)\n",
    "            })\n",
    "            \n",
    "            new_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            split_docs.append(new_doc)\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "# Split the loaded documents\n",
    "split_docs = split_documents(documents)\n",
    "print(f\"Total chunks created: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffb2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3324506/99417199.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/rojan/anaconda3/envs/raizen/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rojan/anaconda3/envs/raizen/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted at ./vector_store\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def create_vector_store(documents: List[Document]):\n",
    "    \"\"\"Create Chroma vector store with metadata indexing\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    return Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "        persist_directory=\"./vector_store\",\n",
    "        # Remove the explicit metadatas parameter\n",
    "    )\n",
    "\n",
    "# Create and persist vector store\n",
    "vector_store = create_vector_store(split_docs)\n",
    "print(\"Vector store created and persisted at ./vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc86b5",
   "metadata": {},
   "source": [
    "<h1>Retrieving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e9ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced prompt template created in 'prompts' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create prompts directory if not exists\n",
    "PROMPT_DIR = \"prompts\"\n",
    "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
    "\n",
    "# Define and save enhanced skill tree prompt\n",
    "SKILL_TREE_PROMPT = \"\"\"ROLE: Expert Learning Path Designer\n",
    "TASK: Generate structured skill trees in VALID JSON format\n",
    "\n",
    "STRICT FORMAT RULES:\n",
    "1. Use double quotes ONLY\n",
    "2. No markdown/code blocks\n",
    "3. Valid JSON syntax required\n",
    "4. No trailing commas\n",
    "5. Include ALL brackets\n",
    "6. Maintain proper indentation\n",
    "\n",
    "EXAMPLE RESPONSE:\n",
    "{{\n",
    "    \"skill_tree\": {{\n",
    "        \"title\": \"Web Development\",\n",
    "        \"prerequisites\": [\"Basic Computer Skills\"],\n",
    "        \"learning_path\": [\n",
    "            {{\n",
    "                \"order\": 1,\n",
    "                \"topic\": \"HTML Fundamentals\",\n",
    "                \"resources\": [\"MDN Web Docs\", \"FreeCodeCamp HTML Course\"],\n",
    "                \"milestones\": [\"Build basic page structure\", \"Create semantic markup\"]\n",
    "            }},\n",
    "            {{\n",
    "                \"order\": 2,\n",
    "                \"topic\": \"CSS Styling\",\n",
    "                \"resources\": [\"CSS Tricks Guide\", \"Flexbox Froggy\"],\n",
    "                \"milestones\": [\"Style responsive layouts\", \"Implement CSS animations\"]\n",
    "            }}\n",
    "        ],\n",
    "        \"dependencies\": [\"JavaScript\", \"Browser APIs\"]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "ANALYSIS STEPS:\n",
    "1. Identify core concepts from query: {query}\n",
    "2. Extract prerequisites from context: {context}\n",
    "3. Create ordered learning path\n",
    "4. Select relevant resources\n",
    "5. Define clear milestones\n",
    "\n",
    "CONTEXT FROM KNOWLEDGE BASE:\n",
    "{context}\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "RESPONSE (JSON ONLY):\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(PROMPT_DIR, \"skill_tree_generator.txt\"), \"w\") as f:\n",
    "    f.write(SKILL_TREE_PROMPT)\n",
    "\n",
    "print(\"Enhanced prompt template created in 'prompts' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33623f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 7/7 [00:47<00:00,  6.81s/it]\n",
      "/home/rojan/anaconda3/envs/raizen/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Input Validation ===\n",
      "\n",
      "Sample 1:\n",
      "Question: How to learn PostgreSQL from basics? ...\n",
      "GT: PostgreSQL Fundamentals Basic Understanding of Databases Familiarity with SQL Fundamentals Data Definition Language (DDL) Data Manipulation Language (DML) Data Control Language (DCL) Advanced Features...\n",
      "Response: PostgreSQL Learning Path Basic Computer Skills Familiarity with command line (optional) Introduction to Relational Databases and PostgreSQL Basic PostgreSQL Setup and Configuration SQL Fundamentals Ad...\n",
      "\n",
      "Sample 2:\n",
      "Question: How to learn Spring Boot from basics? ...\n",
      "GT: Java and Spring Fundamentals Basic Programming Concepts Understanding of Object-Oriented Programming (OOP) Java Fundamentals Spring Framework Basics Introduction to Spring Boot Web Development Data Ac...\n",
      "Response: Spring Boot Learning Path Basic Java Programming Understanding of Web Concepts Java Fundamentals Spring Core Spring Boot Basics Spring MVC Spring Data JPA Spring Security Spring Boot Actuator Spring C...\n",
      "\n",
      "Sample 3:\n",
      "Question: How to learn Python from basics? ...\n",
      "GT: Python Fundamentals Basic Programming Concepts Fundamentals Object-Oriented Programming (OOP) Advanced Topics Python Libraries Python for Web Development...\n",
      "Response: Python Fundamentals Basic Computer Skills Python Basics Data Structures Functions and Modules Object-Oriented Programming (OOP) Error Handling and Advanced Features Package Management and Libraries Ba...\n",
      "\n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 459.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.05 seconds, 150.57 sentences/sec\n",
      "\n",
      "=== Aggregate Metrics ===\n",
      "BERT Precision: 0.865\n",
      "BERT Recall: 0.890\n",
      "BERT F1: 0.877\n",
      "Average Latency: 6.81s\n",
      "Total Queries: 7\n",
      "Success Rate: 100.0%\n",
      "\n",
      "=== Sample Responses ===\n",
      "\n",
      "Question 1: How to learn PostgreSQL from basics? \n",
      "Latency: 6.85s\n",
      "BERT Precision: 0.854\n",
      "BERT Recall: 0.857\n",
      "BERT F1: 0.856\n",
      "Ground Truth: {'skill_tree': {'title': 'PostgreSQL Fundamentals', 'prerequisites': ['Basic Understanding of Databases', 'Familiarity with SQL'], 'learning_path': [{'order': 1, 'topic': 'Fundamentals', 'resources': ['PostgreSQL Official Documentation', 'SQL Basics Tutorial'], 'milestones': ['Understand relational database concepts', 'Learn SQL basics', 'Explore PostgreSQL features', 'Install and configure PostgreSQL']}, {'order': 2, 'topic': 'Data Definition Language (DDL)', 'resources': ['PostgreSQL DDL Documentation', 'Table Creation Guide'], 'milestones': ['Create and manage tables', 'Work with indexes', 'Create and use views', 'Understand and manage schemas']}, {'order': 3, 'topic': 'Data Manipulation Language (DML)', 'resources': ['PostgreSQL DML Documentation', 'SQL Joins Guide'], 'milestones': ['Query data using SELECT', 'Insert data with INSERT', 'Update data with UPDATE', 'Delete data with DELETE', 'Perform JOIN operations']}, {'order': 4, 'topic': 'Data Control Language (DCL)', 'resources': ['PostgreSQL DCL Documentation', 'User Management Guide'], 'milestones': ['Manage users and roles', 'Set permissions and access controls', 'Understand and implement roles']}, {'order': 5, 'topic': 'Advanced Features', 'resources': ['PostgreSQL Stored Procedures Guide', 'Triggers Documentation', 'Functions Documentation'], 'milestones': ['Create and use stored procedures', 'Implement triggers', 'Write and use functions', 'Manage transactions']}, {'order': 6, 'topic': 'Performance Tuning', 'resources': ['PostgreSQL Performance Tuning Guide', 'EXPLAIN Documentation', 'Index Optimization Guide'], 'milestones': ['Use EXPLAIN to analyze queries', 'Optimize indexes', 'Improve query performance', 'Monitor and use statistics']}], 'dependencies': ['Basic SQL Knowledge', 'Understanding of Database Concepts']}}\n",
      "Model Response: {'skill_tree': {'title': 'PostgreSQL Learning Path', 'prerequisites': ['Basic Computer Skills', 'Familiarity with command line (optional)'], 'learning_path': [{'order': 1, 'topic': 'Introduction to Relational Databases and PostgreSQL', 'resources': ['PostgreSQL Documentation - Introduction', 'Khan Academy - SQL Basics'], 'milestones': ['Understand what a relational database is', 'Differentiate PostgreSQL from other database systems', 'Define RDBMS benefits and limitations']}, {'order': 2, 'topic': 'Basic PostgreSQL Setup and Configuration', 'resources': ['PostgreSQL Documentation - Installation Guide', 'DigitalOcean - How To Install and Use PostgreSQL'], 'milestones': ['Install PostgreSQL on your local machine', 'Connect to PostgreSQL using psql', 'Understand basic configuration parameters']}, {'order': 3, 'topic': 'SQL Fundamentals', 'resources': ['SQLZoo', 'Mode Analytics - SQL Tutorial'], 'milestones': ['Write basic SELECT, INSERT, UPDATE, DELETE queries', 'Define and use common data types', 'Filter and sort data using WHERE and ORDER BY clauses']}, {'order': 4, 'topic': 'Advanced SQL', 'resources': ['PostgreSQL Documentation - SQL Tutorial', 'LeetCode - SQL Problems'], 'milestones': ['Perform JOIN operations to combine data from multiple tables', 'Write subqueries to solve complex data retrieval tasks', 'Use aggregate functions to summarize data', 'Apply window functions for advanced data analysis', 'Understand and use Recursive CTEs']}, {'order': 5, 'topic': 'PostgreSQL Features and Concepts', 'resources': ['PostgreSQL Documentation - Server Administration', '2ndQuadrant Blogs'], 'milestones': ['Write PL/pgSQL functions and procedures', 'Implement database triggers for automation', 'Understand different index types (B-Tree, Hash, GiST, SP-GiST, GIN, BRIN) and their use cases', 'Configure storage parameters for database objects']}, {'order': 6, 'topic': 'Schema Design and SQL Patterns', 'resources': ['Database Design and Relational Theory - C.J. Date', 'Refactoring Databases - Scott Ambler'], 'milestones': ['Apply schema design patterns and avoid anti-patterns', 'Recognize and implement efficient SQL query patterns', 'Optimize database schema for performance and scalability']}, {'order': 7, 'topic': 'PostgreSQL Performance Tuning and Troubleshooting', 'resources': ['PostgreSQL Wiki - Performance Optimization', 'explain.dalibo.com'], 'milestones': ['Tune PostgreSQL for different workloads (OLTP, OLAP, HTAP)', 'Analyze query performance using EXPLAIN, Depesz, and PEV2 Tenser', 'Apply troubleshooting techniques using USE RED and Golden Signals', 'Monitor PostgreSQL using pg_stat_activity and pgcenter', 'Implement SQL optimization techniques']}, {'order': 8, 'topic': 'PostgreSQL Tools and Ecosystem', 'resources': ['Postgres Tools', 'pgAdmin', 'psql command-line tools'], 'milestones': ['Utilize various PostgreSQL tools for administration and monitoring', 'Become proficient with psql command-line interface', 'Explore and use pgAdmin for database management']}, {'order': 9, 'topic': 'Community and Continuous Learning', 'resources': ['PostgreSQL Mailing Lists', 'PostgreSQL Planet'], 'milestones': ['Engage with the PostgreSQL community through mailing lists', 'Contribute to PostgreSQL development by reviewing and writing patches', 'Stay updated with the latest PostgreSQL features and best practices']}], 'dependencies': ['SQL', 'Database Concepts']}}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question 2: How to learn Spring Boot from basics? \n",
      "Latency: 4.71s\n",
      "BERT Precision: 0.868\n",
      "BERT Recall: 0.861\n",
      "BERT F1: 0.865\n",
      "Ground Truth: {'skill_tree': {'title': 'Java and Spring Fundamentals', 'prerequisites': ['Basic Programming Concepts', 'Understanding of Object-Oriented Programming (OOP)'], 'learning_path': [{'order': 1, 'topic': 'Java Fundamentals', 'resources': ['Java Official Documentation', 'Java Tutorials by Oracle'], 'milestones': ['Understand Object-Oriented Programming (OOP)', 'Implement exception handling', 'Use the Collections Framework', 'Work with Lambda Expressions and Stream API']}, {'order': 2, 'topic': 'Spring Framework Basics', 'resources': ['Spring Framework Documentation', 'Spring Dependency Injection Guide'], 'milestones': ['Understand Dependency Injection (DI)', 'Learn Aspect-Oriented Programming (AOP)', 'Work with Spring Beans', 'Configure Spring applications']}, {'order': 3, 'topic': 'Introduction to Spring Boot', 'resources': ['Spring Boot Official Documentation', 'Spring Boot Tutorial'], 'milestones': ['Understand Spring Boot overview', 'Set up Spring Boot project structure', 'Configure Spring Boot applications', 'Use auto-configuration']}, {'order': 4, 'topic': 'Web Development', 'resources': ['Spring MVC Documentation', 'RESTful API Guide', 'Thymeleaf Documentation'], 'milestones': ['Build web applications with Spring MVC', 'Create RESTful APIs', 'Use Thymeleaf as a template engine', 'Implement validation']}, {'order': 5, 'topic': 'Data Access', 'resources': ['JPA/Hibernate Documentation', 'Spring Data JPA Guide'], 'milestones': ['Use JPA/Hibernate for ORM', 'Work with Spring Data JPA', 'Manage transactions', 'Configure database connections']}, {'order': 6, 'topic': 'Security', 'resources': ['Spring Security Documentation', 'OAuth2 Guide'], 'milestones': ['Implement Spring Security', 'Handle authentication and authorization', 'Use OAuth2 for secure APIs', 'Protect against CSRF attacks']}, {'order': 7, 'topic': 'Testing', 'resources': ['JUnit Documentation', 'Mockito Guide', 'Spring Boot Testing Documentation'], 'milestones': ['Write unit tests with JUnit', 'Use Mockito for mocking', 'Test Spring Boot applications', 'Perform integration testing']}, {'order': 8, 'topic': 'Advanced Topics', 'resources': ['Spring Actuators Documentation', 'Caching Strategies Guide', 'RabbitMQ and Kafka Documentation'], 'milestones': ['Use Spring Actuators for monitoring', 'Implement caching strategies', 'Work with messaging systems (RabbitMQ, Kafka)', 'Perform batch processing']}, {'order': 9, 'topic': 'Deployment and Operations', 'resources': ['Maven and Gradle Documentation', 'Docker Official Documentation', 'Cloud Deployment Guides (AWS, Azure, GCP)'], 'milestones': ['Build projects with Maven/Gradle', 'Containerize applications with Docker', 'Deploy to cloud platforms (AWS, Azure, GCP)', 'Set up CI/CD pipelines']}, {'order': 10, 'topic': 'Performance and Scalability', 'resources': ['Java Profiling Tools', 'Load Testing Tools', 'Spring Cloud Documentation'], 'milestones': ['Profile applications for performance', 'Conduct load testing', 'Implement microservices architecture', 'Use Spring Cloud for distributed systems']}], 'dependencies': ['Basic Java Knowledge', 'Understanding of Web Development Concepts']}}\n",
      "Model Response: {'skill_tree': {'title': 'Spring Boot Learning Path', 'prerequisites': ['Basic Java Programming', 'Understanding of Web Concepts'], 'learning_path': [{'order': 1, 'topic': 'Java Fundamentals', 'resources': ['Oracle Java Tutorials', 'Head First Java'], 'milestones': ['Understand OOP concepts', 'Work with collections and streams', 'Handle exceptions']}, {'order': 2, 'topic': 'Spring Core', 'resources': ['Spring Framework Documentation', 'Spring in Action book'], 'milestones': ['Understand Dependency Injection', 'Implement Inversion of Control', 'Explore Spring AOP']}, {'order': 3, 'topic': 'Spring Boot Basics', 'resources': ['Spring Boot Documentation', 'Spring Initializr', 'Baeldung Spring Boot Tutorials'], 'milestones': ['Create a Spring Boot application', 'Understand Spring Boot Starters', 'Implement autoconfiguration']}, {'order': 4, 'topic': 'Spring MVC', 'resources': ['Spring MVC Documentation', 'Spring Web MVC Step-by-Step'], 'milestones': ['Build RESTful APIs', 'Handle HTTP requests and responses', 'Implement controllers and views']}, {'order': 5, 'topic': 'Spring Data JPA', 'resources': ['Spring Data JPA Documentation', 'Hibernate ORM Documentation'], 'milestones': ['Connect to databases', 'Perform CRUD operations', 'Implement JPA repositories']}, {'order': 6, 'topic': 'Spring Security', 'resources': ['Spring Security Documentation', 'Spring Security in Action'], 'milestones': ['Implement authentication and authorization', 'Secure REST APIs', 'Understand OAuth2 and JWT']}, {'order': 7, 'topic': 'Spring Boot Actuator', 'resources': ['Spring Boot Actuator Documentation', 'Microservices with Spring Boot and Spring Cloud'], 'milestones': ['Monitor application health', 'Expose application metrics', 'Manage application via endpoints']}, {'order': 8, 'topic': 'Spring Cloud (Microservices)', 'resources': ['Spring Cloud Documentation', 'Building Microservices with Spring Cloud'], 'milestones': ['Implement service discovery with Eureka', 'Build API Gateway with Spring Cloud Gateway', 'Explore Circuit Breakers']}, {'order': 9, 'topic': 'Testing in Spring Boot', 'resources': ['Spring Boot Testing Documentation', 'JUnit 5 Documentation', 'Mockito Documentation'], 'milestones': ['Write unit tests and integration tests', 'Use Mockito for mocking', 'Perform JPA and MVC testing']}], 'dependencies': ['Java', 'Maven or Gradle', 'Database (e.g., PostgreSQL, MySQL)']}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict\n",
    "\n",
    "class SkillTreeGenerator:\n",
    "    def __init__(self, vector_store, api_key: str):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')\n",
    "        self.retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    def _load_prompt(self, name: str) -> str:\n",
    "        with open(f\"prompts/{name}.txt\") as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def generate_path(self, query: str) -> Dict:\n",
    "        start_time = time.time()\n",
    "        response = None  # Initialize response variable\n",
    "        try:\n",
    "            docs = self.retriever.invoke(query)\n",
    "            context = \"\\n\".join([d.page_content for d in docs])\n",
    "            prompt = self._load_prompt(\"skill_tree_generator\").format(\n",
    "                context=context,\n",
    "                query=query\n",
    "            )\n",
    "            response = self.model.generate_content(prompt)\n",
    "            json_str = response.text.replace('```json', '').replace('```', '')\n",
    "            return {\n",
    "                \"response\": json.loads(json_str),\n",
    "                \"latency\": time.time() - start_time\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"response\": {\"error\": \"Invalid JSON format\"}, \"latency\": time.time() - start_time}\n",
    "        except Exception as e:\n",
    "            # Safely handle missing response\n",
    "            raw_response = getattr(response, 'text', 'No response generated')\n",
    "            print(f\"\\nError processing query: {query}\")\n",
    "            print(f\"Raw response: {raw_response}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            return {\"response\": {\"error\": f\"API Error: {str(e)}\"}, \"latency\": time.time() - start_time}\n",
    "\n",
    "def extract_content(json_str: str) -> str:\n",
    "    \"\"\"Extract semantic content from JSON responses\"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        if \"error\" in data:\n",
    "            return data[\"error\"]\n",
    "        elif \"skill_tree\" in data:\n",
    "            skill_tree = data[\"skill_tree\"]\n",
    "            content = [\n",
    "                skill_tree.get(\"title\", \"\"),\n",
    "                \" \".join(skill_tree.get(\"prerequisites\", [])),\n",
    "                \" \".join([topic.get(\"topic\", \"\") for topic in skill_tree.get(\"learning_path\", [])])\n",
    "            ]\n",
    "            return \" \".join(content).strip()\n",
    "        return json_str\n",
    "    except:\n",
    "        return json_str\n",
    "\n",
    "def main(vector_store):\n",
    "    # Read groundtruth data with proper JSON formatting\n",
    "    df = pd.read_csv('gt.csv').reset_index(drop=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    processor = SkillTreeGenerator(vector_store, \"AIzaSyCRZbC56Mhvw-xv0M3YDG0fbv-Wq3zLBkc\")\n",
    "    \n",
    "    # Generate responses with latency tracking\n",
    "    print(\"Generating model responses...\")\n",
    "    results = []\n",
    "    for question in tqdm(df['Questions'], desc=\"Processing queries\"):\n",
    "        result = processor.generate_path(question)\n",
    "        results.append({\n",
    "            \"Model Response\": json.dumps(result[\"response\"]),\n",
    "            \"Latency\": result[\"latency\"]\n",
    "        })\n",
    "    \n",
    "    # Merge results with alignment fix\n",
    "    results_df = pd.DataFrame(results).reset_index(drop=True)\n",
    "    df = pd.concat([df, results_df], axis=1)\n",
    "    \n",
    "    # Preprocess for BERTScore\n",
    "    df['Processed GT'] = df['Answers'].apply(extract_content)\n",
    "    df['Processed Response'] = df['Model Response'].apply(extract_content)\n",
    "    df['is_error'] = df['Model Response'].str.contains('\"error\":')\n",
    "    \n",
    "    # Sanity check input samples\n",
    "    print(\"\\n=== Input Validation ===\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Question: {df['Questions'].iloc[i][:100]}...\")\n",
    "        print(f\"GT: {df['Processed GT'].iloc[i][:200]}...\")\n",
    "        print(f\"Response: {df['Processed Response'].iloc[i][:200]}...\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    valid_mask = ~df['is_error']\n",
    "    \n",
    "    # BERT Scores for valid responses only\n",
    "    if valid_mask.any():\n",
    "        references = df.loc[valid_mask, 'Processed GT'].tolist()\n",
    "        candidates = df.loc[valid_mask, 'Processed Response'].tolist()\n",
    "        P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "    else:\n",
    "        P, R, F1 = [pd.Series([0.0])] * 3\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n=== Aggregate Metrics ===\")\n",
    "    print(f\"BERT Precision: {P.mean().item():.3f}\" if valid_mask.any() else \"BERT Precision: N/A (all errors)\")\n",
    "    print(f\"BERT Recall: {R.mean().item():.3f}\" if valid_mask.any() else \"BERT Recall: N/A (all errors)\")\n",
    "    print(f\"BERT F1: {F1.mean().item():.3f}\" if valid_mask.any() else \"BERT F1: N/A (all errors)\")\n",
    "    print(f\"Average Latency: {df['Latency'].mean():.2f}s\")\n",
    "    print(f\"Total Queries: {len(df)}\")\n",
    "    print(f\"Success Rate: {(valid_mask.mean() * 100):.1f}%\")\n",
    "    \n",
    "    # Print sample responses\n",
    "    print(\"\\n=== Sample Responses ===\")\n",
    "    for i in range(2):\n",
    "        print(f\"\\nQuestion {i+1}: {df['Questions'].iloc[i]}\")\n",
    "        print(f\"Latency: {df['Latency'].iloc[i]:.2f}s\")\n",
    "        if valid_mask.any() and i < len(P):\n",
    "            print(f\"BERT Precision: {P[i].item():.3f}\")\n",
    "            print(f\"BERT Recall: {R[i].item():.3f}\")\n",
    "            print(f\"BERT F1: {F1[i].item():.3f}\")\n",
    "        print(\"Ground Truth:\", json.loads(df['Answers'].iloc[i]))\n",
    "        print(\"Model Response:\", json.loads(df['Model Response'].iloc[i]))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569e80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raizen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
